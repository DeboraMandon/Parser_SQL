{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DéboraMandon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELECT * FROM question WHERE q_id IN (SELECT r_fk_question_id FROM reponse WHERE r_date_ajout BETWEEN  2013-01-01  AND  2013-12-31 )']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialiser les données textes\n",
    "txt = \"SELECT * FROM question WHERE q_id IN (SELECT r_fk_question_id FROM reponse WHERE r_date_ajout BETWEEN  2013-01-01  AND  2013-12-31 )\"\n",
    "\n",
    "# Appliquer la tokenisation\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "tokenizer.tokenize(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SELECT', '*', 'FROM', 'question', 'WHERE', 'q_id', 'IN', '(', 'SELECT', 'r_fk_question_id', 'FROM', 'reponse', 'WHERE', 'r_date_ajout', 'BETWEEN', '2013-01-01', 'AND', '2013-12-31', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "mots= word_tokenize(txt, language='french')\n",
    "\n",
    "print(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avais', 'le', 'tu', 'ayant', 'et', 'sois', 'ton', 'l', 'je', 'avons', 'y', 'fusses', 'avait', 'aurez', 'étée', 'aies', 'soyez', 'fussent', 'serez', 'toi', 'étants', 'avec', 'soyons', 'eue', 'ayons', 'ils', 'êtes', 'la', 'qu', 'eurent', 'son', 't', 'étions', 'aurait', 'leur', 'fussions', 'auront', 'eu', 'aviez', 'aux', 'es', 'même', 'eussiez', 'eusse', 'ait', 'ayants', 'aura', 'auraient', 'avaient', 'votre', 'sont', 'étais', 'fus', 'nous', 'sera', 'ta', 'était', 'avions', 'est', 'serons', 'pas', 's', 'j', 'se', 'aurai', 'ces', 'étantes', 'étées', 'eûmes', 'de', 'dans', 'les', 'fûtes', 'c', 'en', 'ayante', 'eut', 'étiez', 'suis', 'ou', 'ses', 'as', 'étant', 'mais', 'une', 'ayantes', 'elle', 'seras', 'sa', 'tes', 'un', 'serai', 'eus', 'seront', 'seriez', 'étés', 'm', 'à', 'étaient', 'été', 'aient', 'aurions', 'n', 'sommes', 'eux', 'ma', 'eût', 'que', 'fûmes', 'auras', 'serions', 'aie', 'avez', 'ai', 'eussent', 'eussions', 'vos', 'lui', 'ayez', 'qui', 'au', 'vous', 'mon', 'nos', 'serais', 'moi', 'fussiez', 'sur', 'furent', 'eues', 'aurais', 'ne', 'me', 'fût', 'fut', 'soient', 'des', 'ont', 'notre', 'soit', 'seraient', 'étante', 'il', 'pour', 'serait', 'fusse', 'ce', 'te', 'eusses', 'mes', 'on', 'du', 'auriez', 'aurons', 'd', 'eûtes', 'par'}\n"
     ]
    }
   ],
   "source": [
    "# Importer stopwords de la classe nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialiser la variable des mots vides\n",
    "stop_words = set(stopwords.words('french'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'q_id', 'r_fk_question_id', 'reponse', 'r_date_ajout', '2013-01-01', '2013-12-31']\n"
     ]
    }
   ],
   "source": [
    "# ajouter les mots vides : \".\" et \",\"\n",
    "stop_words.update([\"SELECT\", \"*\", \"WHERE\", 'IN', \"(\", \")\", \"BETWEEN\", \"AND\", \"FROM\"])\n",
    "\n",
    "# définir la fonction stop_words_filtering\n",
    "def stop_words_filtering(mots) :\n",
    "   tokens = []\n",
    "   for mot in mots:\n",
    "       if mot not in stop_words:\n",
    "           tokens.append(mot)\n",
    "   return tokens\n",
    "\n",
    "# Appliquer la fonction stop_words_filtering à la variable mots\n",
    "print(stop_words_filtering(mots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select', 'from', 'question', 'where', 'select', 'question', 'from', 'reponse', 'where', 'date', 'ajout', 'between']\n"
     ]
    }
   ],
   "source": [
    "#Importer la package nécessaire\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "# Initialiser un tokenisteur\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Zé]{4,}\")\n",
    "\n",
    "# Calculer les tokens\n",
    "tokens = tokenizer.tokenize(txt.lower())\n",
    "\n",
    "# Afficher les tokens\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
